{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "70e1c5fc-1460-4c4d-87fc-1e71e396fd4d",
   "metadata": {},
   "outputs": [],
   "source": "# Twitter Cultural Product Fit Analyzer\n## Analyzing Product-Market Fit Across NYC, London, and Paris\n\nThis project analyzes Twitter data from three major cities to identify cultural preferences and recommend optimal product-market matches for marketing campaigns."
  },
  {
   "cell_type": "markdown",
   "id": "5bycefgwaxq",
   "source": "## 1. Data Loading and Initial Exploration\n\nLet's start by loading all four datasets and understanding their structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "axv1qzlnya9",
   "source": "# Import necessary libraries\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "sjq1csh4fsi",
   "source": "# Load all datasets\ndef load_twitter_data(filename):\n    \"\"\"Load Twitter JSON data and return as list of dictionaries\"\"\"\n    with open(filename, 'r', encoding='utf-8') as f:\n        # Read line by line since each line is a separate JSON object\n        data = []\n        for line in f:\n            try:\n                tweet = json.loads(line.strip())\n                data.append(tweet)\n            except json.JSONDecodeError:\n                continue\n    return data\n\n# Load all datasets\nprint(\"Loading datasets...\")\nrandom_tweets = load_twitter_data('random_tweets.json')\nnew_york_tweets = load_twitter_data('new_york.json')\nlondon_tweets = load_twitter_data('london.json')\nparis_tweets = load_twitter_data('paris.json')\n\nprint(f\"✓ Random tweets loaded: {len(random_tweets):,} tweets\")\nprint(f\"✓ New York tweets loaded: {len(new_york_tweets):,} tweets\")\nprint(f\"✓ London tweets loaded: {len(london_tweets):,} tweets\")\nprint(f\"✓ Paris tweets loaded: {len(paris_tweets):,} tweets\")\nprint(f\"\\nTotal tweets across all datasets: {len(random_tweets) + len(new_york_tweets) + len(london_tweets) + len(paris_tweets):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8g33p4uf1i9",
   "source": "### 1.1 Exploring Data Structure\n\nLet's examine the structure of a sample tweet to understand what fields are available.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vg8komrkcal",
   "source": "# Examine the structure of a sample tweet\nprint(\"Sample tweet structure from New York dataset:\")\nprint(\"-\" * 50)\n\n# Pretty print the first tweet\nsample_tweet = new_york_tweets[0]\nfor key in sorted(sample_tweet.keys()):\n    value = sample_tweet[key]\n    if isinstance(value, str) and len(value) > 50:\n        value = value[:50] + \"...\"\n    print(f\"{key}: {value}\")\n    \nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Check if all datasets have similar structure\nprint(\"Checking if all datasets have similar fields...\")\nrandom_keys = set(random_tweets[0].keys()) if random_tweets else set()\nny_keys = set(new_york_tweets[0].keys()) if new_york_tweets else set()\nlondon_keys = set(london_tweets[0].keys()) if london_tweets else set()\nparis_keys = set(paris_tweets[0].keys()) if paris_tweets else set()\n\nall_keys = random_keys | ny_keys | london_keys | paris_keys\nprint(f\"Total unique fields across all datasets: {len(all_keys)}\")\n\n# Check for differences\nif random_keys == ny_keys == london_keys == paris_keys:\n    print(\"✓ All datasets have the same structure!\")\nelse:\n    print(\"⚠ Datasets have different structures. Investigating differences...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ym43cnpnjpm",
   "source": "### 1.2 Key Fields for Marketing Analysis\n\nLet's identify the most important fields for our cultural product fit analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2n5ra33dje3",
   "source": "# Extract key fields for analysis\ndef extract_key_fields(tweet):\n    \"\"\"Extract the most relevant fields for marketing analysis\"\"\"\n    return {\n        'text': tweet.get('text', ''),\n        'created_at': tweet.get('created_at', ''),\n        'user_location': tweet.get('user', {}).get('location', ''),\n        'user_description': tweet.get('user', {}).get('description', ''),\n        'user_followers': tweet.get('user', {}).get('followers_count', 0),\n        'user_friends': tweet.get('user', {}).get('friends_count', 0),\n        'user_verified': tweet.get('user', {}).get('verified', False),\n        'retweet_count': tweet.get('retweet_count', 0),\n        'favorite_count': tweet.get('favorite_count', 0),\n        'lang': tweet.get('lang', ''),\n        'hashtags': [tag['text'] for tag in tweet.get('entities', {}).get('hashtags', [])],\n        'user_mentions': [mention['screen_name'] for mention in tweet.get('entities', {}).get('user_mentions', [])],\n        'place_name': tweet.get('place', {}).get('name', '') if tweet.get('place') else '',\n        'place_country': tweet.get('place', {}).get('country', '') if tweet.get('place') else ''\n    }\n\n# Convert to DataFrames for easier analysis\nprint(\"Converting to DataFrames...\")\ndf_random = pd.DataFrame([extract_key_fields(tweet) for tweet in random_tweets])\ndf_ny = pd.DataFrame([extract_key_fields(tweet) for tweet in new_york_tweets])\ndf_london = pd.DataFrame([extract_key_fields(tweet) for tweet in london_tweets])\ndf_paris = pd.DataFrame([extract_key_fields(tweet) for tweet in paris_tweets])\n\n# Add city labels\ndf_random['city'] = 'Random'\ndf_ny['city'] = 'New York'\ndf_london['city'] = 'London'\ndf_paris['city'] = 'Paris'\n\nprint(\"✓ DataFrames created successfully!\")\nprint(f\"\\nDataFrame shapes:\")\nprint(f\"Random: {df_random.shape}\")\nprint(f\"New York: {df_ny.shape}\")\nprint(f\"London: {df_london.shape}\")\nprint(f\"Paris: {df_paris.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x8tmx16s2cg",
   "source": "### 1.3 Basic Statistics and Data Quality Check",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "su8fjszfm6q",
   "source": "# Combine all city dataframes for comparison\ndf_cities = pd.concat([df_ny, df_london, df_paris], ignore_index=True)\n\nprint(\"=== BASIC STATISTICS ===\\n\")\n\n# Language distribution\nprint(\"Language Distribution by City:\")\nlang_dist = df_cities.groupby(['city', 'lang']).size().unstack(fill_value=0)\nprint(lang_dist.head(10))\nprint(f\"\\nTop languages: {df_cities['lang'].value_counts().head(5).to_dict()}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Missing data check\nprint(\"Missing Data Analysis:\")\nfor df, name in [(df_ny, 'New York'), (df_london, 'London'), (df_paris, 'Paris')]:\n    missing_text = df['text'].isna().sum()\n    empty_text = (df['text'] == '').sum()\n    print(f\"{name}: {missing_text} missing texts, {empty_text} empty texts\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# User engagement statistics\nprint(\"User Engagement Statistics by City:\")\nengagement_stats = df_cities.groupby('city')[['retweet_count', 'favorite_count', 'user_followers']].agg(['mean', 'median', 'std'])\nprint(engagement_stats.round(2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pg1as0k6h2",
   "source": "### 1.4 Visualizing Tweet Characteristics Across Cities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xhqkz4gjgua",
   "source": "# Create visualization of key metrics\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Tweet Characteristics Across Cities', fontsize=16)\n\n# 1. Language distribution\nax1 = axes[0, 0]\ntop_langs = df_cities['lang'].value_counts().head(5).index\nlang_by_city = df_cities[df_cities['lang'].isin(top_langs)].groupby(['city', 'lang']).size().unstack(fill_value=0)\nlang_by_city.plot(kind='bar', ax=ax1)\nax1.set_title('Top 5 Languages by City')\nax1.set_xlabel('City')\nax1.set_ylabel('Number of Tweets')\nax1.legend(title='Language', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# 2. Tweet length distribution\nax2 = axes[0, 1]\ndf_cities['text_length'] = df_cities['text'].str.len()\nfor city in ['New York', 'London', 'Paris']:\n    city_data = df_cities[df_cities['city'] == city]['text_length']\n    ax2.hist(city_data, bins=50, alpha=0.5, label=city, density=True)\nax2.set_title('Tweet Length Distribution')\nax2.set_xlabel('Character Count')\nax2.set_ylabel('Density')\nax2.legend()\nax2.set_xlim(0, 300)\n\n# 3. Engagement metrics\nax3 = axes[1, 0]\nengagement_data = df_cities.groupby('city')[['retweet_count', 'favorite_count']].mean()\nengagement_data.plot(kind='bar', ax=ax3)\nax3.set_title('Average Engagement by City')\nax3.set_xlabel('City')\nax3.set_ylabel('Average Count')\nax3.legend(['Retweets', 'Favorites'])\nax3.tick_params(axis='x', rotation=45)\n\n# 4. Posting time analysis (extract hour from created_at)\nax4 = axes[1, 1]\n# Parse datetime and extract hour\nfor df in [df_ny, df_london, df_paris]:\n    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n\nhour_dist = pd.concat([\n    df_ny['hour'].value_counts().sort_index(),\n    df_london['hour'].value_counts().sort_index(),\n    df_paris['hour'].value_counts().sort_index()\n], axis=1, keys=['New York', 'London', 'Paris'])\n\nhour_dist.plot(ax=ax4, marker='o')\nax4.set_title('Tweet Activity by Hour of Day')\nax4.set_xlabel('Hour (UTC)')\nax4.set_ylabel('Number of Tweets')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y7ssvynondp",
   "source": "### 1.5 Sample Tweets Analysis\n\nLet's look at some sample tweets from each city to get a feel for the content.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b9uc8oxx3ye",
   "source": "# Display sample tweets from each city\nprint(\"=== SAMPLE TWEETS FROM EACH CITY ===\\n\")\n\n# Function to clean and display tweets\ndef display_sample_tweets(df, city_name, n=5):\n    print(f\"\\n{city_name.upper()} - Sample Tweets:\")\n    print(\"-\" * 80)\n    \n    # Filter for English tweets with reasonable length\n    english_tweets = df[(df['lang'] == 'en') & (df['text'].str.len() > 50)]\n    \n    if len(english_tweets) == 0:\n        print(f\"No English tweets found. Showing from all languages:\")\n        english_tweets = df[df['text'].str.len() > 50]\n    \n    # Random sample\n    sample = english_tweets.sample(n=min(n, len(english_tweets)), random_state=42)\n    \n    for idx, (_, tweet) in enumerate(sample.iterrows(), 1):\n        text = tweet['text'].replace('\\n', ' ')\n        print(f\"\\n{idx}. Tweet: {text[:200]}...\")\n        print(f\"   Language: {tweet['lang']} | Retweets: {tweet['retweet_count']} | Favorites: {tweet['favorite_count']}\")\n        if tweet['hashtags']:\n            print(f\"   Hashtags: {', '.join(tweet['hashtags'][:5])}\")\n\n# Display samples\nfor df, city in [(df_ny, 'New York'), (df_london, 'London'), (df_paris, 'Paris')]:\n    display_sample_tweets(df, city, n=3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cm9uam9i16",
   "source": "### 1.6 Initial Insights for Marketing\n\nBased on our exploration, let's summarize key insights relevant for marketing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wwg3ov9qon",
   "source": "# Summary statistics for marketing insights\nprint(\"=== KEY MARKETING INSIGHTS FROM INITIAL EXPLORATION ===\\n\")\n\n# 1. Market size comparison\nprint(\"1. MARKET SIZE (Tweet Volume):\")\nmarket_size = pd.DataFrame({\n    'City': ['New York', 'London', 'Paris'],\n    'Total Tweets': [len(df_ny), len(df_london), len(df_paris)],\n    'English Tweets': [\n        len(df_ny[df_ny['lang'] == 'en']),\n        len(df_london[df_london['lang'] == 'en']),\n        len(df_paris[df_paris['lang'] == 'en'])\n    ]\n})\nmarket_size['English %'] = (market_size['English Tweets'] / market_size['Total Tweets'] * 100).round(1)\nprint(market_size)\n\nprint(\"\\n2. USER INFLUENCE METRICS (Average per city):\")\ninfluence_metrics = df_cities.groupby('city').agg({\n    'user_followers': ['mean', 'median'],\n    'user_verified': 'sum',\n    'retweet_count': 'mean',\n    'favorite_count': 'mean'\n}).round(2)\nprint(influence_metrics)\n\nprint(\"\\n3. CONTENT CHARACTERISTICS:\")\n# Average tweet length by city\navg_length = df_cities.groupby('city')['text_length'].mean().round(1)\nprint(f\"Average tweet length by city:\\n{avg_length}\")\n\nprint(\"\\n4. PEAK ACTIVITY HOURS (UTC):\")\n# Find peak hours for each city\nfor city in ['New York', 'London', 'Paris']:\n    city_df = df_cities[df_cities['city'] == city]\n    if 'hour' in city_df.columns:\n        peak_hour = city_df['hour'].mode().values[0] if len(city_df['hour'].mode()) > 0 else 'N/A'\n        print(f\"{city}: Peak hour is {peak_hour}:00 UTC\")\n\nprint(\"\\n5. HASHTAG USAGE:\")\n# Count hashtag usage by city\nhashtag_usage = df_cities.groupby('city').apply(\n    lambda x: sum(len(tags) for tags in x['hashtags'])\n).to_dict()\nfor city, count in hashtag_usage.items():\n    avg_hashtags = count / len(df_cities[df_cities['city'] == city])\n    print(f\"{city}: {avg_hashtags:.2f} hashtags per tweet\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25f7l8qgaxy",
   "source": "## Next Steps\n\nNow that we've explored the data, we understand:\n- The structure and quality of our datasets\n- Key differences between cities in terms of language, engagement, and activity patterns\n- Potential features for our cultural product fit analysis\n\nIn the next sections, we will:\n1. Clean and preprocess the text data\n2. Extract product and brand mentions\n3. Analyze cultural preferences\n4. Build our product-market fit recommendation system",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}